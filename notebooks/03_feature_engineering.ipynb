{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db1b947",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now that data is cleaned, we create statistical and temporal features from the sensor readings to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d107073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "train_clean = pd.read_csv('../data/processed/train_ops_cleaned.csv')\n",
    "val_clean = pd.read_csv('../data/processed/val_ops_cleaned.csv')\n",
    "\n",
    "# Load target variables\n",
    "train_tte = pd.read_csv('../data/raw/train_tte.csv')\n",
    "val_labels = pd.read_csv('../data/raw/validation_labels.csv')\n",
    "\n",
    "print(f\"Training set: {train_clean.shape}\")\n",
    "print(f\"Validation set: {val_clean.shape}\")\n",
    "print(f\"\\nTarget variable (training): {train_tte.shape}\")\n",
    "print(f\"Target variable (validation): {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data structure\n",
    "print(\"Training data structure:\")\n",
    "print(train_clean.head())\n",
    "print(f\"\\nShape: {train_clean.shape}\")\n",
    "print(f\"Unique vehicles: {train_clean['vehicle_id'].nunique()}\")\n",
    "print(f\"\\nRows per vehicle (sample):\")\n",
    "print(train_clean.groupby('vehicle_id').size().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442ee75",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440981a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistical_features(df, sensor_cols):\n",
    "    \"\"\"\n",
    "    Create statistical aggregation features from sensor readings.\n",
    "    \n",
    "    Groups time-series data by vehicle_id and computes:\n",
    "    - Mean: Average sensor value across all time steps\n",
    "    - Median: Middle value (robust to outliers)\n",
    "    - Std: Standard deviation (measure of volatility)\n",
    "    - Min: Minimum sensor reading\n",
    "    - Max: Maximum sensor reading\n",
    "    \"\"\"\n",
    "    agg_dict = {}\n",
    "    \n",
    "    for col in sensor_cols:\n",
    "        if col in df.columns and col not in ['vehicle_id', 'time_step']:\n",
    "            agg_dict[f'{col}_mean'] = (col, 'mean')\n",
    "            agg_dict[f'{col}_median'] = (col, 'median')\n",
    "            agg_dict[f'{col}_std'] = (col, 'std')\n",
    "            agg_dict[f'{col}_min'] = (col, 'min')\n",
    "            agg_dict[f'{col}_max'] = (col, 'max')\n",
    "    \n",
    "    # Group by vehicle and aggregate\n",
    "    features = df.groupby('vehicle_id').agg(**agg_dict).reset_index()\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def create_temporal_features(df, sensor_cols):\n",
    "    \"\"\"\n",
    "    Create temporal features from time-series sensor data.\n",
    "    \n",
    "    For each vehicle, compute:\n",
    "    - Last: Most recent sensor reading (final time step)\n",
    "    - Trend: Linear slope of sensor values over time\n",
    "    - Volatility: Standard deviation of sensor values\n",
    "    \"\"\"\n",
    "    temporal_features = []\n",
    "    \n",
    "    # Focus on cumulative sensors for trend calculation\n",
    "    cumulative_sensors = [col for col in sensor_cols if '167_' in col]\n",
    "    \n",
    "    for vehicle_id, group in df.groupby('vehicle_id'):\n",
    "        vehicle_features = {'vehicle_id': vehicle_id}\n",
    "        \n",
    "        for col in cumulative_sensors:\n",
    "            if col in group.columns:\n",
    "                values = group[col].values\n",
    "                \n",
    "                # Last value (most recent reading)\n",
    "                vehicle_features[f'{col}_last'] = values[-1]\n",
    "                \n",
    "                # Trend: Linear regression slope (change per time step)\n",
    "                if len(values) > 1:\n",
    "                    x = np.arange(len(values))\n",
    "                    slope = np.polyfit(x, values, 1)[0]\n",
    "                    vehicle_features[f'{col}_trend'] = slope\n",
    "                else:\n",
    "                    vehicle_features[f'{col}_trend'] = 0\n",
    "                \n",
    "                # Volatility: Standard deviation\n",
    "                vehicle_features[f'{col}_volatility'] = np.std(values)\n",
    "        \n",
    "        temporal_features.append(vehicle_features)\n",
    "    \n",
    "    return pd.DataFrame(temporal_features)\n",
    "\n",
    "print(\"Feature engineering functions defined\")\n",
    "print(\"\\nStatistical features: mean, median, std, min, max (aggregated across time steps)\")\n",
    "print(\"Temporal features: last value, trend (slope), volatility (for cumulative sensors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sensor columns (exclude vehicle_id)\n",
    "sensor_cols = [col for col in train_clean.columns if col != 'vehicle_id']\n",
    "\n",
    "print(f\"Total sensor columns: {len(sensor_cols)}\")\n",
    "print(f\"Sample sensors: {sensor_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e187c",
   "metadata": {},
   "source": [
    "## Create Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be465095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create statistical features for training data\n",
    "print(\"Creating statistical features for training data...\")\n",
    "train_statistical = create_statistical_features(train_clean, sensor_cols)\n",
    "\n",
    "print(f\"Statistical features shape: {train_statistical.shape}\")\n",
    "print(f\"Sample features: {train_statistical.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features for training data\n",
    "print(\"Creating temporal features for training data...\")\n",
    "train_temporal = create_temporal_features(train_clean, sensor_cols)\n",
    "\n",
    "print(f\"Temporal features shape: {train_temporal.shape}\")\n",
    "print(f\"Sample temporal features: {[c for c in train_temporal.columns if 'trend' in c][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea617d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features for training data\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Merge statistical and temporal features on vehicle_id\n",
    "train_features = train_statistical.merge(train_temporal, on='vehicle_id', how='inner')\n",
    "\n",
    "# Add target variable\n",
    "train_features = train_features.merge(train_tte[['vehicle_id', 'in_study_repair']], \n",
    "                                      on='vehicle_id', how='inner')\n",
    "\n",
    "print(f\"Final training features shape: {train_features.shape}\")\n",
    "print(f\"Total engineered features: {train_features.shape[1] - 2}\")  # Exclude vehicle_id and target\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_features['in_study_repair'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb930141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training features\n",
    "output_path = '../data/features/train_features.csv'\n",
    "train_features.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Training features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9991e",
   "metadata": {},
   "source": [
    "## Validation Data Feature Engineering\n",
    "\n",
    "Apply the same feature engineering transformations to validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create statistical features for validation data\n",
    "print(\"Creating statistical features for validation data...\")\n",
    "val_statistical = create_statistical_features(val_clean, sensor_cols)\n",
    "\n",
    "print(f\"Validation statistical features shape: {val_statistical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features for validation data\n",
    "print(\"Creating temporal features for validation data...\")\n",
    "val_temporal = create_temporal_features(val_clean, sensor_cols)\n",
    "\n",
    "print(f\"Validation temporal features shape: {val_temporal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a315fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features for validation data\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Merge statistical and temporal features\n",
    "val_features = val_statistical.merge(val_temporal, on='vehicle_id', how='inner')\n",
    "\n",
    "# Add target variable (validation uses 'class_label' instead of 'in_study_repair')\n",
    "val_features = val_features.merge(val_labels[['vehicle_id', 'class_label']], \n",
    "                                  on='vehicle_id', how='inner')\n",
    "\n",
    "# Rename to match training data\n",
    "val_features.rename(columns={'class_label': 'in_study_repair'}, inplace=True)\n",
    "\n",
    "print(f\"Final validation features shape: {val_features.shape}\")\n",
    "print(f\"Total engineered features: {val_features.shape[1] - 2}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(val_features['in_study_repair'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e37334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation features\n",
    "output_path = '../data/features/val_features.csv'\n",
    "val_features.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Validation features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d11841",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "We have successfully engineered features from the cleaned sensor data:\n",
    "\n",
    "**Statistical Features:**\n",
    "- Mean: Average sensor value across readings\n",
    "- Median: Middle value, robust to outliers\n",
    "- Standard Deviation: Measure of sensor value variability\n",
    "- Min/Max: Range of sensor readings\n",
    "\n",
    "**Temporal Features:**\n",
    "- Trend: Rate of change for cumulative sensors (167_X series)\n",
    "- Last: Most recent sensor reading\n",
    "- Volatility: Variability measure for cumulative sensors\n",
    "\n",
    "**Output:**\n",
    "- Training features: `data/features/train_features.csv`\n",
    "- Validation features: `data/features/val_features.csv`\n",
    "\n",
    "These engineered features will be used for model training and evaluation in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcf2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339d9fd",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now that data is cleaned, we create statistical and temporal features from the sensor readings to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned operational readouts...\n",
      "âœ… Training data loaded: (1122452, 107)\n",
      "   - Total time series rows: 1,122,452\n",
      "   - Unique vehicles: 23,550\n",
      "   - Sensors: 105\n",
      "\n",
      "âœ… Validation data loaded: (196227, 107)\n",
      "   - Total time series rows: 196,227\n",
      "   - Unique vehicles: 5,046\n",
      "\n",
      "ðŸ” Verification:\n",
      "   - Training missing values: 0\n",
      "   - Validation missing values: 0\n",
      "âœ… Training data loaded: (1122452, 107)\n",
      "   - Total time series rows: 1,122,452\n",
      "   - Unique vehicles: 23,550\n",
      "   - Sensors: 105\n",
      "\n",
      "âœ… Validation data loaded: (196227, 107)\n",
      "   - Total time series rows: 196,227\n",
      "   - Unique vehicles: 5,046\n",
      "\n",
      "ðŸ” Verification:\n",
      "   - Training missing values: 0\n",
      "   - Validation missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "train_clean = pd.read_csv('../data/processed/train_ops_cleaned.csv')\n",
    "val_clean = pd.read_csv('../data/processed/val_ops_cleaned.csv')\n",
    "\n",
    "# Load target variables\n",
    "train_tte = pd.read_csv('../data/raw/train_tte.csv')\n",
    "val_labels = pd.read_csv('../data/raw/validation_labels.csv')\n",
    "\n",
    "print(f\"Training set: {train_clean.shape}\")\n",
    "print(f\"Validation set: {val_clean.shape}\")\n",
    "print(f\"\\nTarget variable (training): {train_tte.shape}\")\n",
    "print(f\"Target variable (validation): {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b374f7d",
   "metadata": {},
   "source": [
    "## Data Structure Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Column breakdown:\n",
      "   - ID columns: 2 ['vehicle_id', 'time_step']\n",
      "   - Sensor columns: 105\n",
      "\n",
      "ðŸŽ¯ Features we'll create:\n",
      "   - 105 sensors Ã— 6 statistics = 630 features\n",
      "\n",
      "ðŸ“‹ Example - Vehicle 0:\n",
      "   - Number of readings: 172\n",
      "   - Time span: 11.2 to 507.4\n",
      "\n",
      "   Sensor 171_0 values (first 5): [167985.0, 167985.0, 331635.0, 354975.0, 365550.0]\n"
     ]
    }
   ],
   "source": [
    "# Check data structure\n",
    "print(\"Training data structure:\")\n",
    "print(train_clean.head())\n",
    "print(f\"\\nShape: {train_clean.shape}\")\n",
    "print(f\"Unique vehicles: {train_clean['vehicle_id'].nunique()}\")\n",
    "print(f\"\\nRows per vehicle (sample):\")\n",
    "print(train_clean.groupby('vehicle_id').size().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b137a",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions\n",
    "\n",
    "Creating statistical aggregation features from sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature extraction function defined!\n",
      "\n",
      " For each vehicle, this creates:\n",
      "   - 1 vehicle_id column\n",
      "   - 105 Ã— 6 = 630 feature columns\n",
      "   - Total: 631 columns\n"
     ]
    }
   ],
   "source": [
    "def create_statistical_features(df, sensor_cols):\n",
    "    \"\"\"\n",
    "    Create statistical aggregation features from sensor readings.\n",
    "    \n",
    "    Groups time-series data by vehicle_id and computes:\n",
    "    - Mean: Average sensor value across all time steps\n",
    "    - Median: Middle value (robust to outliers)\n",
    "    - Std: Standard deviation (measure of volatility)\n",
    "    - Min: Minimum sensor reading\n",
    "    - Max: Maximum sensor reading\n",
    "    \"\"\"\n",
    "    agg_dict = {}\n",
    "    \n",
    "    for col in sensor_cols:\n",
    "        if col in df.columns and col not in ['vehicle_id', 'time_step']:\n",
    "            agg_dict[f'{col}_mean'] = (col, 'mean')\n",
    "            agg_dict[f'{col}_median'] = (col, 'median')\n",
    "            agg_dict[f'{col}_std'] = (col, 'std')\n",
    "            agg_dict[f'{col}_min'] = (col, 'min')\n",
    "            agg_dict[f'{col}_max'] = (col, 'max')\n",
    "    \n",
    "    # Group by vehicle and aggregate\n",
    "    features = df.groupby('vehicle_id').agg(**agg_dict).reset_index()\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def create_temporal_features(df, sensor_cols):\n",
    "    \"\"\"\n",
    "    Create temporal features from time-series sensor data.\n",
    "    \n",
    "    For each vehicle, compute:\n",
    "    - Last: Most recent sensor reading (final time step)\n",
    "    - Trend: Linear slope of sensor values over time\n",
    "    - Volatility: Standard deviation of sensor values\n",
    "    \"\"\"\n",
    "    temporal_features = []\n",
    "    \n",
    "    # Focus on cumulative sensors for trend calculation\n",
    "    cumulative_sensors = [col for col in sensor_cols if '167_' in col]\n",
    "    \n",
    "    for vehicle_id, group in df.groupby('vehicle_id'):\n",
    "        vehicle_features = {'vehicle_id': vehicle_id}\n",
    "        \n",
    "        for col in cumulative_sensors:\n",
    "            if col in group.columns:\n",
    "                values = group[col].values\n",
    "                \n",
    "                # Last value (most recent reading)\n",
    "                vehicle_features[f'{col}_last'] = values[-1]\n",
    "                \n",
    "                # Trend: Linear regression slope (change per time step)\n",
    "                if len(values) > 1:\n",
    "                    x = np.arange(len(values))\n",
    "                    slope = np.polyfit(x, values, 1)[0]\n",
    "                    vehicle_features[f'{col}_trend'] = slope\n",
    "                else:\n",
    "                    vehicle_features[f'{col}_trend'] = 0\n",
    "                \n",
    "                # Volatility: Standard deviation\n",
    "                vehicle_features[f'{col}_volatility'] = np.std(values)\n",
    "        \n",
    "        temporal_features.append(vehicle_features)\n",
    "    \n",
    "    return pd.DataFrame(temporal_features)\n",
    "\n",
    "print(\"Feature engineering functions defined\")\n",
    "print(\"\\nStatistical features: mean, median, std, min, max (aggregated across time steps)\")\n",
    "print(\"Temporal features: last value, trend (slope), volatility (for cumulative sensors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c12a2",
   "metadata": {},
   "source": [
    "## Get Sensor Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341684d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing on Vehicle 0:\n",
      "   - Number of readings: 172\n",
      "\n",
      " Extracted 631 features!\n",
      "\n",
      " Example features for sensor 171_0:\n",
      "   - 171_0_mean:  5,310,002\n",
      "   - 171_0_max:   10,189,950\n",
      "   - 171_0_min:   167,985\n",
      "   - 171_0_std:   2,950,191\n",
      "   - 171_0_last:  10,189,950\n",
      "   - 171_0_trend: 59,299.24 (per time step)\n",
      "\n",
      " This vehicle's 172 readings â†’ 1 row with 631 features\n"
     ]
    }
   ],
   "source": [
    "# Get sensor columns (exclude vehicle_id)\n",
    "sensor_cols = [col for col in train_clean.columns if col != 'vehicle_id']\n",
    "\n",
    "print(f\"Total sensor columns: {len(sensor_cols)}\")\n",
    "print(f\"Sample sensors: {sensor_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0ac4c",
   "metadata": {},
   "source": [
    "## Create Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting features for all training vehicles...\n",
      "Processing 23,550 vehicles...\n",
      "\n",
      "   Processed 1,000 / 23,550 vehicles (4.2%)\n",
      "   Processed 1,000 / 23,550 vehicles (4.2%)\n",
      "   Processed 2,000 / 23,550 vehicles (8.5%)\n",
      "   Processed 2,000 / 23,550 vehicles (8.5%)\n",
      "   Processed 3,000 / 23,550 vehicles (12.7%)\n",
      "   Processed 3,000 / 23,550 vehicles (12.7%)\n",
      "   Processed 4,000 / 23,550 vehicles (17.0%)\n",
      "   Processed 4,000 / 23,550 vehicles (17.0%)\n",
      "   Processed 5,000 / 23,550 vehicles (21.2%)\n",
      "   Processed 5,000 / 23,550 vehicles (21.2%)\n",
      "   Processed 6,000 / 23,550 vehicles (25.5%)\n",
      "   Processed 6,000 / 23,550 vehicles (25.5%)\n",
      "   Processed 7,000 / 23,550 vehicles (29.7%)\n",
      "   Processed 7,000 / 23,550 vehicles (29.7%)\n",
      "   Processed 8,000 / 23,550 vehicles (34.0%)\n",
      "   Processed 8,000 / 23,550 vehicles (34.0%)\n",
      "   Processed 9,000 / 23,550 vehicles (38.2%)\n",
      "   Processed 9,000 / 23,550 vehicles (38.2%)\n",
      "   Processed 10,000 / 23,550 vehicles (42.5%)\n",
      "   Processed 10,000 / 23,550 vehicles (42.5%)\n",
      "   Processed 11,000 / 23,550 vehicles (46.7%)\n",
      "   Processed 11,000 / 23,550 vehicles (46.7%)\n",
      "   Processed 12,000 / 23,550 vehicles (51.0%)\n",
      "   Processed 12,000 / 23,550 vehicles (51.0%)\n",
      "   Processed 13,000 / 23,550 vehicles (55.2%)\n",
      "   Processed 13,000 / 23,550 vehicles (55.2%)\n",
      "   Processed 14,000 / 23,550 vehicles (59.4%)\n",
      "   Processed 14,000 / 23,550 vehicles (59.4%)\n",
      "   Processed 15,000 / 23,550 vehicles (63.7%)\n",
      "   Processed 15,000 / 23,550 vehicles (63.7%)\n",
      "   Processed 16,000 / 23,550 vehicles (67.9%)\n",
      "   Processed 16,000 / 23,550 vehicles (67.9%)\n",
      "   Processed 17,000 / 23,550 vehicles (72.2%)\n",
      "   Processed 17,000 / 23,550 vehicles (72.2%)\n",
      "   Processed 18,000 / 23,550 vehicles (76.4%)\n",
      "   Processed 18,000 / 23,550 vehicles (76.4%)\n",
      "   Processed 19,000 / 23,550 vehicles (80.7%)\n",
      "   Processed 19,000 / 23,550 vehicles (80.7%)\n",
      "   Processed 20,000 / 23,550 vehicles (84.9%)\n",
      "   Processed 20,000 / 23,550 vehicles (84.9%)\n",
      "   Processed 21,000 / 23,550 vehicles (89.2%)\n",
      "   Processed 21,000 / 23,550 vehicles (89.2%)\n",
      "   Processed 22,000 / 23,550 vehicles (93.4%)\n",
      "   Processed 22,000 / 23,550 vehicles (93.4%)\n",
      "   Processed 23,000 / 23,550 vehicles (97.7%)\n",
      "   Processed 23,000 / 23,550 vehicles (97.7%)\n",
      "\n",
      " Feature extraction complete!\n",
      "\n",
      " Feature extraction complete!\n",
      "\n",
      " Results:\n",
      "   - Original: 1,122,452 rows (time series)\n",
      "   - Transformed: 23,550 rows (one per vehicle)\n",
      "   - Features: 631 columns\n",
      "\n",
      "First few rows:\n",
      "\n",
      " Results:\n",
      "   - Original: 1,122,452 rows (time series)\n",
      "   - Transformed: 23,550 rows (one per vehicle)\n",
      "   - Features: 631 columns\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>171_0_mean</th>\n",
       "      <th>171_0_max</th>\n",
       "      <th>171_0_min</th>\n",
       "      <th>171_0_std</th>\n",
       "      <th>171_0_last</th>\n",
       "      <th>171_0_trend</th>\n",
       "      <th>666_0_mean</th>\n",
       "      <th>666_0_max</th>\n",
       "      <th>666_0_min</th>\n",
       "      <th>666_0_std</th>\n",
       "      <th>666_0_last</th>\n",
       "      <th>666_0_trend</th>\n",
       "      <th>427_0_mean</th>\n",
       "      <th>427_0_max</th>\n",
       "      <th>427_0_min</th>\n",
       "      <th>427_0_std</th>\n",
       "      <th>427_0_last</th>\n",
       "      <th>427_0_trend</th>\n",
       "      <th>837_0_mean</th>\n",
       "      <th>837_0_max</th>\n",
       "      <th>837_0_min</th>\n",
       "      <th>837_0_std</th>\n",
       "      <th>837_0_last</th>\n",
       "      <th>837_0_trend</th>\n",
       "      <th>...</th>\n",
       "      <th>397_31_trend</th>\n",
       "      <th>397_32_mean</th>\n",
       "      <th>397_32_max</th>\n",
       "      <th>397_32_min</th>\n",
       "      <th>397_32_std</th>\n",
       "      <th>397_32_last</th>\n",
       "      <th>397_32_trend</th>\n",
       "      <th>397_33_mean</th>\n",
       "      <th>397_33_max</th>\n",
       "      <th>397_33_min</th>\n",
       "      <th>397_33_std</th>\n",
       "      <th>397_33_last</th>\n",
       "      <th>397_33_trend</th>\n",
       "      <th>397_34_mean</th>\n",
       "      <th>397_34_max</th>\n",
       "      <th>397_34_min</th>\n",
       "      <th>397_34_std</th>\n",
       "      <th>397_34_last</th>\n",
       "      <th>397_34_trend</th>\n",
       "      <th>397_35_mean</th>\n",
       "      <th>397_35_max</th>\n",
       "      <th>397_35_min</th>\n",
       "      <th>397_35_std</th>\n",
       "      <th>397_35_last</th>\n",
       "      <th>397_35_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.310002e+06</td>\n",
       "      <td>10189950.0</td>\n",
       "      <td>167985.0</td>\n",
       "      <td>2.950191e+06</td>\n",
       "      <td>10189950.0</td>\n",
       "      <td>59299.241091</td>\n",
       "      <td>197902.168605</td>\n",
       "      <td>372685.0</td>\n",
       "      <td>10787.0</td>\n",
       "      <td>106675.103986</td>\n",
       "      <td>372685.0</td>\n",
       "      <td>2141.220576</td>\n",
       "      <td>9.603583e+07</td>\n",
       "      <td>143970419.0</td>\n",
       "      <td>7413813.0</td>\n",
       "      <td>2.735438e+07</td>\n",
       "      <td>108090562.0</td>\n",
       "      <td>3.635412e+05</td>\n",
       "      <td>27056.273256</td>\n",
       "      <td>41670.0</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>11627.258764</td>\n",
       "      <td>41670.0</td>\n",
       "      <td>222.374078</td>\n",
       "      <td>...</td>\n",
       "      <td>19811.216871</td>\n",
       "      <td>2.273773e+06</td>\n",
       "      <td>4290245.0</td>\n",
       "      <td>261904.0</td>\n",
       "      <td>1.168752e+06</td>\n",
       "      <td>4290245.0</td>\n",
       "      <td>23482.075818</td>\n",
       "      <td>560261.418605</td>\n",
       "      <td>1027186.0</td>\n",
       "      <td>93172.0</td>\n",
       "      <td>268904.723691</td>\n",
       "      <td>1027186.0</td>\n",
       "      <td>5404.486193</td>\n",
       "      <td>74468.220930</td>\n",
       "      <td>138939.0</td>\n",
       "      <td>17874.0</td>\n",
       "      <td>35128.589437</td>\n",
       "      <td>138939.0</td>\n",
       "      <td>704.825745</td>\n",
       "      <td>640.848837</td>\n",
       "      <td>792.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>97.259305</td>\n",
       "      <td>792.0</td>\n",
       "      <td>1.895385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.374853e+06</td>\n",
       "      <td>5648790.0</td>\n",
       "      <td>65520.0</td>\n",
       "      <td>1.582802e+06</td>\n",
       "      <td>5648790.0</td>\n",
       "      <td>161280.982620</td>\n",
       "      <td>192030.212121</td>\n",
       "      <td>289371.0</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>81823.391210</td>\n",
       "      <td>289371.0</td>\n",
       "      <td>8029.846925</td>\n",
       "      <td>1.412227e+08</td>\n",
       "      <td>233978571.0</td>\n",
       "      <td>2891746.0</td>\n",
       "      <td>6.575075e+07</td>\n",
       "      <td>233978571.0</td>\n",
       "      <td>6.686840e+06</td>\n",
       "      <td>40504.787879</td>\n",
       "      <td>68717.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>17424.536858</td>\n",
       "      <td>68717.0</td>\n",
       "      <td>1713.963235</td>\n",
       "      <td>...</td>\n",
       "      <td>34666.067179</td>\n",
       "      <td>1.042075e+06</td>\n",
       "      <td>1680728.0</td>\n",
       "      <td>10388.0</td>\n",
       "      <td>4.739484e+05</td>\n",
       "      <td>1680728.0</td>\n",
       "      <td>47628.890709</td>\n",
       "      <td>222924.727273</td>\n",
       "      <td>328910.0</td>\n",
       "      <td>2413.0</td>\n",
       "      <td>92176.751548</td>\n",
       "      <td>328910.0</td>\n",
       "      <td>8893.321858</td>\n",
       "      <td>44156.727273</td>\n",
       "      <td>57511.0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>15670.098558</td>\n",
       "      <td>57511.0</td>\n",
       "      <td>1356.959559</td>\n",
       "      <td>674.484848</td>\n",
       "      <td>793.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>196.619016</td>\n",
       "      <td>793.0</td>\n",
       "      <td>14.790107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.274506e+06</td>\n",
       "      <td>7603590.0</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>2.457206e+06</td>\n",
       "      <td>7603590.0</td>\n",
       "      <td>119071.893863</td>\n",
       "      <td>102097.492958</td>\n",
       "      <td>230831.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>77016.246497</td>\n",
       "      <td>230831.0</td>\n",
       "      <td>3726.522837</td>\n",
       "      <td>1.175975e+08</td>\n",
       "      <td>273813365.0</td>\n",
       "      <td>111133.0</td>\n",
       "      <td>8.798317e+07</td>\n",
       "      <td>273813365.0</td>\n",
       "      <td>4.231729e+06</td>\n",
       "      <td>45315.323944</td>\n",
       "      <td>100121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33425.292786</td>\n",
       "      <td>100121.0</td>\n",
       "      <td>1613.823608</td>\n",
       "      <td>...</td>\n",
       "      <td>11413.280483</td>\n",
       "      <td>1.146652e+06</td>\n",
       "      <td>2648967.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>8.722539e+05</td>\n",
       "      <td>2648967.0</td>\n",
       "      <td>42250.217404</td>\n",
       "      <td>228721.971831</td>\n",
       "      <td>531790.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>176474.445392</td>\n",
       "      <td>531790.0</td>\n",
       "      <td>8544.124413</td>\n",
       "      <td>31454.070423</td>\n",
       "      <td>76915.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>25077.855652</td>\n",
       "      <td>76915.0</td>\n",
       "      <td>1209.976761</td>\n",
       "      <td>443.070423</td>\n",
       "      <td>712.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>193.095198</td>\n",
       "      <td>712.0</td>\n",
       "      <td>9.289906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 631 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id    171_0_mean   171_0_max  171_0_min     171_0_std  171_0_last  \\\n",
       "0           0  5.310002e+06  10189950.0   167985.0  2.950191e+06  10189950.0   \n",
       "1           2  3.374853e+06   5648790.0    65520.0  1.582802e+06   5648790.0   \n",
       "2           3  3.274506e+06   7603590.0     1530.0  2.457206e+06   7603590.0   \n",
       "\n",
       "     171_0_trend     666_0_mean  666_0_max  666_0_min      666_0_std  \\\n",
       "0   59299.241091  197902.168605   372685.0    10787.0  106675.103986   \n",
       "1  161280.982620  192030.212121   289371.0     2226.0   81823.391210   \n",
       "2  119071.893863  102097.492958   230831.0       28.0   77016.246497   \n",
       "\n",
       "   666_0_last  666_0_trend    427_0_mean    427_0_max  427_0_min  \\\n",
       "0    372685.0  2141.220576  9.603583e+07  143970419.0  7413813.0   \n",
       "1    289371.0  8029.846925  1.412227e+08  233978571.0  2891746.0   \n",
       "2    230831.0  3726.522837  1.175975e+08  273813365.0   111133.0   \n",
       "\n",
       "      427_0_std   427_0_last   427_0_trend    837_0_mean  837_0_max  \\\n",
       "0  2.735438e+07  108090562.0  3.635412e+05  27056.273256    41670.0   \n",
       "1  6.575075e+07  233978571.0  6.686840e+06  40504.787879    68717.0   \n",
       "2  8.798317e+07  273813365.0  4.231729e+06  45315.323944   100121.0   \n",
       "\n",
       "   837_0_min     837_0_std  837_0_last  837_0_trend  ...  397_31_trend  \\\n",
       "0     2296.0  11627.258764     41670.0   222.374078  ...  19811.216871   \n",
       "1      424.0  17424.536858     68717.0  1713.963235  ...  34666.067179   \n",
       "2        0.0  33425.292786    100121.0  1613.823608  ...  11413.280483   \n",
       "\n",
       "    397_32_mean  397_32_max  397_32_min    397_32_std  397_32_last  \\\n",
       "0  2.273773e+06   4290245.0    261904.0  1.168752e+06    4290245.0   \n",
       "1  1.042075e+06   1680728.0     10388.0  4.739484e+05    1680728.0   \n",
       "2  1.146652e+06   2648967.0       221.0  8.722539e+05    2648967.0   \n",
       "\n",
       "   397_32_trend    397_33_mean  397_33_max  397_33_min     397_33_std  \\\n",
       "0  23482.075818  560261.418605   1027186.0     93172.0  268904.723691   \n",
       "1  47628.890709  222924.727273    328910.0      2413.0   92176.751548   \n",
       "2  42250.217404  228721.971831    531790.0        40.0  176474.445392   \n",
       "\n",
       "   397_33_last  397_33_trend   397_34_mean  397_34_max  397_34_min  \\\n",
       "0    1027186.0   5404.486193  74468.220930    138939.0     17874.0   \n",
       "1     328910.0   8893.321858  44156.727273     57511.0      1081.0   \n",
       "2     531790.0   8544.124413  31454.070423     76915.0       296.0   \n",
       "\n",
       "     397_34_std  397_34_last  397_34_trend  397_35_mean  397_35_max  \\\n",
       "0  35128.589437     138939.0    704.825745   640.848837       792.0   \n",
       "1  15670.098558      57511.0   1356.959559   674.484848       793.0   \n",
       "2  25077.855652      76915.0   1209.976761   443.070423       712.0   \n",
       "\n",
       "   397_35_min  397_35_std  397_35_last  397_35_trend  \n",
       "0       452.0   97.259305        792.0      1.895385  \n",
       "1        88.0  196.619016        793.0     14.790107  \n",
       "2       165.0  193.095198        712.0      9.289906  \n",
       "\n",
       "[3 rows x 631 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create statistical features for training data\n",
    "print(\"Creating statistical features for training data...\")\n",
    "train_statistical = create_statistical_features(train_clean, sensor_cols)\n",
    "\n",
    "print(f\"Statistical features shape: {train_statistical.shape}\")\n",
    "print(f\"Sample features: {train_statistical.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c825c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features for training data\n",
    "print(\"Creating temporal features for training data...\")\n",
    "train_temporal = create_temporal_features(train_clean, sensor_cols)\n",
    "\n",
    "print(f\"Temporal features shape: {train_temporal.shape}\")\n",
    "print(f\"Sample temporal features: {[c for c in train_temporal.columns if 'trend' in c][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b90ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting features for validation vehicles...\n",
      "Processing 5,046 vehicles...\n",
      "\n",
      "   Processed 500 / 5,046 vehicles (9.9%)\n",
      "   Processed 500 / 5,046 vehicles (9.9%)\n",
      "   Processed 1,000 / 5,046 vehicles (19.8%)\n",
      "   Processed 1,000 / 5,046 vehicles (19.8%)\n",
      "   Processed 1,500 / 5,046 vehicles (29.7%)\n",
      "   Processed 1,500 / 5,046 vehicles (29.7%)\n",
      "   Processed 2,000 / 5,046 vehicles (39.6%)\n",
      "   Processed 2,000 / 5,046 vehicles (39.6%)\n",
      "   Processed 2,500 / 5,046 vehicles (49.5%)\n",
      "   Processed 2,500 / 5,046 vehicles (49.5%)\n",
      "   Processed 3,000 / 5,046 vehicles (59.5%)\n",
      "   Processed 3,000 / 5,046 vehicles (59.5%)\n",
      "   Processed 3,500 / 5,046 vehicles (69.4%)\n",
      "   Processed 3,500 / 5,046 vehicles (69.4%)\n",
      "   Processed 4,000 / 5,046 vehicles (79.3%)\n",
      "   Processed 4,000 / 5,046 vehicles (79.3%)\n",
      "   Processed 4,500 / 5,046 vehicles (89.2%)\n",
      "   Processed 4,500 / 5,046 vehicles (89.2%)\n",
      "   Processed 5,000 / 5,046 vehicles (99.1%)\n",
      "   Processed 5,000 / 5,046 vehicles (99.1%)\n",
      "\n",
      " Validation feature extraction complete!\n",
      "\n",
      " Validation feature extraction complete!\n",
      "\n",
      " Results:\n",
      "   - Original: 196,227 rows (time series)\n",
      "   - Transformed: 5,046 rows (one per vehicle)\n",
      "   - Features: 631 columns\n",
      "\n",
      " Results:\n",
      "   - Original: 196,227 rows (time series)\n",
      "   - Transformed: 5,046 rows (one per vehicle)\n",
      "   - Features: 631 columns\n"
     ]
    }
   ],
   "source": [
    "# Combine all features for training data\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Merge statistical and temporal features on vehicle_id\n",
    "train_features = train_statistical.merge(train_temporal, on='vehicle_id', how='inner')\n",
    "\n",
    "# Add target variable\n",
    "train_features = train_features.merge(train_tte[['vehicle_id', 'in_study_repair']], \n",
    "                                      on='vehicle_id', how='inner')\n",
    "\n",
    "print(f\"Final training features shape: {train_features.shape}\")\n",
    "print(f\"Total engineered features: {train_features.shape[1] - 2}\")  # Exclude vehicle_id and target\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_features['in_study_repair'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training features\n",
    "output_path = '../data/features/train_features.csv'\n",
    "train_features.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Training features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e6e27",
   "metadata": {},
   "source": [
    "## Validation Data Feature Engineering\n",
    "\n",
    "Apply the same feature engineering transformations to validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78632d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (23550, 632)\n",
      "Training label distribution:\n",
      "in_study_repair\n",
      "0    21278\n",
      "1     2272\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set shape: (5046, 632)\n",
      "Validation label distribution:\n",
      "in_study_repair\n",
      "0    4910\n",
      "1     136\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create statistical features for validation data\n",
    "print(\"Creating statistical features for validation data...\")\n",
    "val_statistical = create_statistical_features(val_clean, sensor_cols)\n",
    "\n",
    "print(f\"Validation statistical features shape: {val_statistical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved train_features.csv\n",
      "âœ“ Saved val_features.csv\n",
      "\n",
      "Feature engineering complete! Ready for model training.\n"
     ]
    }
   ],
   "source": [
    "# Create temporal features for validation data\n",
    "print(\"Creating temporal features for validation data...\")\n",
    "val_temporal = create_temporal_features(val_clean, sensor_cols)\n",
    "\n",
    "print(f\"Validation temporal features shape: {val_temporal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216c012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting trend features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract trend features\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting trend features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m trend_features = \u001b[43mfeature_engine\u001b[49m.extract_trend_features(train_ops)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrend features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrend_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m display(trend_features.head())\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all features for validation data\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Merge statistical and temporal features\n",
    "val_features = val_statistical.merge(val_temporal, on='vehicle_id', how='inner')\n",
    "\n",
    "# Add target variable (validation uses 'class_label' instead of 'in_study_repair')\n",
    "val_features = val_features.merge(val_labels[['vehicle_id', 'class_label']], \n",
    "                                  on='vehicle_id', how='inner')\n",
    "\n",
    "# Rename to match training data\n",
    "val_features.rename(columns={'class_label': 'in_study_repair'}, inplace=True)\n",
    "\n",
    "print(f\"Final validation features shape: {val_features.shape}\")\n",
    "print(f\"Total engineered features: {val_features.shape[1] - 2}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(val_features['in_study_repair'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation features\n",
    "output_path = '../data/features/val_features.csv'\n",
    "val_features.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Validation features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c012a",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "We have successfully engineered features from the cleaned sensor data:\n",
    "\n",
    "**Statistical Features:**\n",
    "- Mean: Average sensor value across readings\n",
    "- Median: Middle value, robust to outliers\n",
    "- Standard Deviation: Measure of sensor value variability\n",
    "- Min/Max: Range of sensor readings\n",
    "\n",
    "**Temporal Features:**\n",
    "- Trend: Rate of change for cumulative sensors (167_X series)\n",
    "- Last: Most recent sensor reading\n",
    "- Volatility: Variability measure for cumulative sensors\n",
    "\n",
    "**Output:**\n",
    "- Training features: `data/features/train_features.csv`\n",
    "- Validation features: `data/features/val_features.csv`\n",
    "\n",
    "These engineered features will be used for model training and evaluation in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
