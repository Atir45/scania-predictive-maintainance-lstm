{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdd87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing - Handling Missing Values\n",
    "# ================================================\n",
    "# This notebook handles missing data in the operational readouts\n",
    "# We use median imputation to fill 0.3% missing values in 167_X sensors\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer  # Keep for benchmarking comparison only\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d387af",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "**What we're doing:** Loading the operational readouts that have missing values\n",
    "\n",
    "**Why:** Need to examine and fix missing data before feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e9f5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training operational readouts...\n",
      "‚úÖ Loaded training data: (1122452, 107)\n",
      "   - Rows (time steps): 1,122,452\n",
      "   - Columns (sensors): 107\n",
      "   - Unique vehicles: 23,550\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>171_0</th>\n",
       "      <th>666_0</th>\n",
       "      <th>427_0</th>\n",
       "      <th>837_0</th>\n",
       "      <th>167_0</th>\n",
       "      <th>167_1</th>\n",
       "      <th>167_2</th>\n",
       "      <th>167_3</th>\n",
       "      <th>167_4</th>\n",
       "      <th>167_5</th>\n",
       "      <th>167_6</th>\n",
       "      <th>167_7</th>\n",
       "      <th>167_8</th>\n",
       "      <th>167_9</th>\n",
       "      <th>309_0</th>\n",
       "      <th>272_0</th>\n",
       "      <th>272_1</th>\n",
       "      <th>272_2</th>\n",
       "      <th>272_3</th>\n",
       "      <th>272_4</th>\n",
       "      <th>272_5</th>\n",
       "      <th>272_6</th>\n",
       "      <th>272_7</th>\n",
       "      <th>...</th>\n",
       "      <th>397_11</th>\n",
       "      <th>397_12</th>\n",
       "      <th>397_13</th>\n",
       "      <th>397_14</th>\n",
       "      <th>397_15</th>\n",
       "      <th>397_16</th>\n",
       "      <th>397_17</th>\n",
       "      <th>397_18</th>\n",
       "      <th>397_19</th>\n",
       "      <th>397_20</th>\n",
       "      <th>397_21</th>\n",
       "      <th>397_22</th>\n",
       "      <th>397_23</th>\n",
       "      <th>397_24</th>\n",
       "      <th>397_25</th>\n",
       "      <th>397_26</th>\n",
       "      <th>397_27</th>\n",
       "      <th>397_28</th>\n",
       "      <th>397_29</th>\n",
       "      <th>397_30</th>\n",
       "      <th>397_31</th>\n",
       "      <th>397_32</th>\n",
       "      <th>397_33</th>\n",
       "      <th>397_34</th>\n",
       "      <th>397_35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>167985.0</td>\n",
       "      <td>10787.0</td>\n",
       "      <td>7413813.0</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>4110.0</td>\n",
       "      <td>1296420.0</td>\n",
       "      <td>1628265.0</td>\n",
       "      <td>630345.0</td>\n",
       "      <td>1269525.0</td>\n",
       "      <td>4772940.0</td>\n",
       "      <td>2706706.0</td>\n",
       "      <td>222225.0</td>\n",
       "      <td>6240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1435083.0</td>\n",
       "      <td>857662.0</td>\n",
       "      <td>384579.0</td>\n",
       "      <td>668642.0</td>\n",
       "      <td>7239843.0</td>\n",
       "      <td>398490.0</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>53161.0</td>\n",
       "      <td>178881.0</td>\n",
       "      <td>138250.0</td>\n",
       "      <td>13328.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>16361.0</td>\n",
       "      <td>131601.0</td>\n",
       "      <td>116541.0</td>\n",
       "      <td>13506.0</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6337.0</td>\n",
       "      <td>105412.0</td>\n",
       "      <td>95728.0</td>\n",
       "      <td>15609.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>150228.0</td>\n",
       "      <td>261904.0</td>\n",
       "      <td>93172.0</td>\n",
       "      <td>17874.0</td>\n",
       "      <td>452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>167985.0</td>\n",
       "      <td>10787.0</td>\n",
       "      <td>7413813.0</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>1302855.0</td>\n",
       "      <td>1628265.0</td>\n",
       "      <td>630345.0</td>\n",
       "      <td>1269526.0</td>\n",
       "      <td>4772940.0</td>\n",
       "      <td>2706706.0</td>\n",
       "      <td>222225.0</td>\n",
       "      <td>6240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1440661.0</td>\n",
       "      <td>857662.0</td>\n",
       "      <td>384579.0</td>\n",
       "      <td>668642.0</td>\n",
       "      <td>7239843.0</td>\n",
       "      <td>398490.0</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>53210.0</td>\n",
       "      <td>178883.0</td>\n",
       "      <td>138252.0</td>\n",
       "      <td>13328.0</td>\n",
       "      <td>3582.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>16368.0</td>\n",
       "      <td>131601.0</td>\n",
       "      <td>116542.0</td>\n",
       "      <td>13507.0</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6339.0</td>\n",
       "      <td>105413.0</td>\n",
       "      <td>95729.0</td>\n",
       "      <td>15610.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>150228.0</td>\n",
       "      <td>261905.0</td>\n",
       "      <td>93172.0</td>\n",
       "      <td>17874.0</td>\n",
       "      <td>452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>331635.0</td>\n",
       "      <td>14525.0</td>\n",
       "      <td>13683604.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1787736.0</td>\n",
       "      <td>1133132.0</td>\n",
       "      <td>598351.0</td>\n",
       "      <td>1167062.0</td>\n",
       "      <td>12314224.0</td>\n",
       "      <td>460240.0</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>232.0</td>\n",
       "      <td>75038.0</td>\n",
       "      <td>352791.0</td>\n",
       "      <td>327992.0</td>\n",
       "      <td>17325.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>24028.0</td>\n",
       "      <td>234737.0</td>\n",
       "      <td>216619.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>3476.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12055.0</td>\n",
       "      <td>167693.0</td>\n",
       "      <td>142900.0</td>\n",
       "      <td>19263.0</td>\n",
       "      <td>2441.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>204832.0</td>\n",
       "      <td>313485.0</td>\n",
       "      <td>106464.0</td>\n",
       "      <td>19306.0</td>\n",
       "      <td>452.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id  time_step     171_0    666_0       427_0   837_0   167_0  \\\n",
       "0           0       11.2  167985.0  10787.0   7413813.0  2296.0  4110.0   \n",
       "1           0       11.4  167985.0  10787.0   7413813.0  2296.0  4111.0   \n",
       "2           0       19.6  331635.0  14525.0  13683604.0  2600.0     NaN   \n",
       "\n",
       "       167_1      167_2     167_3      167_4      167_5      167_6     167_7  \\\n",
       "0  1296420.0  1628265.0  630345.0  1269525.0  4772940.0  2706706.0  222225.0   \n",
       "1  1302855.0  1628265.0  630345.0  1269526.0  4772940.0  2706706.0  222225.0   \n",
       "2        NaN        NaN       NaN        NaN        NaN        NaN       NaN   \n",
       "\n",
       "    167_8  167_9  309_0      272_0      272_1     272_2      272_3  \\\n",
       "0  6240.0    0.0   70.0  1435083.0   857662.0  384579.0   668642.0   \n",
       "1  6240.0    0.0   70.0  1440661.0   857662.0  384579.0   668642.0   \n",
       "2     NaN    NaN   70.0  1787736.0  1133132.0  598351.0  1167062.0   \n",
       "\n",
       "        272_4     272_5   272_6  272_7  ...  397_11   397_12    397_13  \\\n",
       "0   7239843.0  398490.0  3887.0    0.0  ...   224.0  53161.0  178881.0   \n",
       "1   7239843.0  398490.0  3887.0    0.0  ...   224.0  53210.0  178883.0   \n",
       "2  12314224.0  460240.0  3887.0    0.0  ...   232.0  75038.0  352791.0   \n",
       "\n",
       "     397_14   397_15  397_16  397_17   397_18    397_19    397_20   397_21  \\\n",
       "0  138250.0  13328.0  3581.0    88.0  16361.0  131601.0  116541.0  13506.0   \n",
       "1  138252.0  13328.0  3582.0    88.0  16368.0  131601.0  116542.0  13507.0   \n",
       "2  327992.0  17325.0  4451.0    92.0  24028.0  234737.0  216619.0  17000.0   \n",
       "\n",
       "   397_22  397_23   397_24    397_25    397_26   397_27  397_28  397_29  \\\n",
       "0  2856.0    48.0   6337.0  105412.0   95728.0  15609.0  1984.0     8.0   \n",
       "1  2856.0    48.0   6339.0  105413.0   95729.0  15610.0  1984.0     8.0   \n",
       "2  3476.0    48.0  12055.0  167693.0  142900.0  19263.0  2441.0    12.0   \n",
       "\n",
       "   397_30    397_31    397_32    397_33   397_34  397_35  \n",
       "0   784.0  150228.0  261904.0   93172.0  17874.0   452.0  \n",
       "1   784.0  150228.0  261905.0   93172.0  17874.0   452.0  \n",
       "2  1420.0  204832.0  313485.0  106464.0  19306.0   452.0  \n",
       "\n",
       "[3 rows x 107 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Load training operational readouts\n",
    "# This is the large file (1.2GB) with time series sensor data\n",
    "print(\"Loading training operational readouts...\")\n",
    "train_ops = pd.read_csv('../data/raw/train_operational_readouts.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded training data: {train_ops.shape}\")\n",
    "print(f\"   - Rows (time steps): {len(train_ops):,}\")\n",
    "print(f\"   - Columns (sensors): {len(train_ops.columns)}\")\n",
    "print(f\"   - Unique vehicles: {train_ops['vehicle_id'].nunique():,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(train_ops.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d1d2b",
   "metadata": {},
   "source": [
    "## 2. Analyze Missing Data\n",
    "\n",
    "**What we're doing:** Finding which columns have missing values and how many\n",
    "\n",
    "**Expected:** ~0.3% missing in 167_X columns (cumulative sensors like mileage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3a1f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing missing data patterns...\n",
      "\n",
      " Missing Data Summary:\n",
      "   - Total cells: 120,102,364\n",
      "   - Missing cells: 354,634\n",
      "   - Missing percentage: 0.2953%\n",
      "   - Columns affected: 104\n",
      "\n",
      " Columns with missing values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291_1</td>\n",
       "      <td>9628</td>\n",
       "      <td>0.857765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>291_3</td>\n",
       "      <td>9628</td>\n",
       "      <td>0.857765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>291_0</td>\n",
       "      <td>9628</td>\n",
       "      <td>0.857765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291_10</td>\n",
       "      <td>9628</td>\n",
       "      <td>0.857765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291_9</td>\n",
       "      <td>9628</td>\n",
       "      <td>0.857765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>272_5</td>\n",
       "      <td>525</td>\n",
       "      <td>0.046773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>666_0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.003564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>837_0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.003475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>835_0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.003475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>309_0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.003029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Column  Missing Count  Missing %\n",
       "0     291_1           9628   0.857765\n",
       "1     291_3           9628   0.857765\n",
       "2     291_0           9628   0.857765\n",
       "3    291_10           9628   0.857765\n",
       "4     291_9           9628   0.857765\n",
       "..      ...            ...        ...\n",
       "99    272_5            525   0.046773\n",
       "100   666_0             40   0.003564\n",
       "101   837_0             39   0.003475\n",
       "102   835_0             39   0.003475\n",
       "103   309_0             34   0.003029\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 2: Count missing values in each column\n",
    "print(\"Analyzing missing data patterns...\\n\")\n",
    "\n",
    "# Count NaN values per column\n",
    "missing_counts = train_ops.isnull().sum()\n",
    "missing_pct = (missing_counts / len(train_ops) * 100).sort_values(ascending=False)\n",
    "\n",
    "# Filter to show only columns with missing data\n",
    "cols_with_missing = missing_pct[missing_pct > 0]\n",
    "\n",
    "print(f\" Missing Data Summary:\")\n",
    "print(f\"   - Total cells: {train_ops.size:,}\")\n",
    "print(f\"   - Missing cells: {train_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing percentage: {(train_ops.isnull().sum().sum() / train_ops.size * 100):.4f}%\")\n",
    "print(f\"   - Columns affected: {len(cols_with_missing)}\")\n",
    "\n",
    "print(f\"\\n Columns with missing values:\")\n",
    "display(pd.DataFrame({\n",
    "    'Column': cols_with_missing.index,\n",
    "    'Missing Count': missing_counts[cols_with_missing.index].values,\n",
    "    'Missing %': cols_with_missing.values\n",
    "}).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7d67a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column breakdown:\n",
      "   - ID columns: 2 ['vehicle_id', 'time_step']\n",
      "   - Sensor columns: 105\n",
      "\n",
      "We'll only impute sensor columns (not IDs)\n",
      "\n",
      "Example - Row 2 of sensor 167_1:\n",
      "   Value: nan\n",
      "     This is NaN (missing) - we'll fill it with median\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Prepare for imputation\n",
    "# Separate ID columns from sensor columns\n",
    "id_cols = ['vehicle_id', 'time_step']\n",
    "sensor_cols = [col for col in train_ops.columns if col not in id_cols]\n",
    "\n",
    "print(f\"Column breakdown:\")\n",
    "print(f\"   - ID columns: {len(id_cols)} {id_cols}\")\n",
    "print(f\"   - Sensor columns: {len(sensor_cols)}\")\n",
    "print(f\"\\nWe'll only impute sensor columns (not IDs)\")\n",
    "\n",
    "# Show example of missing data\n",
    "print(f\"\\nExample - Row 2 of sensor 167_1:\")\n",
    "print(f\"   Value: {train_ops.loc[2, '167_1']}\")\n",
    "if pd.isna(train_ops.loc[2, '167_1']):\n",
    "    print(f\"     This is NaN (missing) - we'll fill it with median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d89a5",
   "metadata": {},
   "source": [
    "---\n",
    "### Option A: Custom Implementation (USED IN FINAL MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39cd4ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CUSTOM: Calculating median for each sensor column...\n",
      "‚úÖ Custom medians calculated!\n",
      "\n",
      "üìà Learned medians for 105 sensors\n",
      "\n",
      "Example custom medians:\n",
      "   - 171_0: 2,781,472\n",
      "   - 666_0: 76,455\n",
      "   - 427_0: 108,090,562\n",
      "   - 837_0: 15,755\n",
      "   - 167_0: 3,570\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM IMPLEMENTATION: Calculate medians manually (no sklearn)\n",
    "print(\"üìä CUSTOM: Calculating median for each sensor column...\")\n",
    "\n",
    "# Dictionary to store median values for each sensor\n",
    "sensor_medians_custom = {}\n",
    "\n",
    "# Calculate median for each sensor column (ignoring NaN values)\n",
    "for col in sensor_cols:\n",
    "    # Get non-missing values for this column\n",
    "    non_missing_values = train_ops[col].dropna().values\n",
    "    \n",
    "    # Calculate median manually using NumPy\n",
    "    if len(non_missing_values) > 0:\n",
    "        sensor_medians_custom[col] = np.median(non_missing_values)\n",
    "    else:\n",
    "        sensor_medians_custom[col] = 0  # Fallback if all values are missing\n",
    "        \n",
    "print(\"‚úÖ Custom medians calculated!\")\n",
    "print(f\"\\nüìà Learned medians for {len(sensor_medians_custom)} sensors\")\n",
    "print(f\"\\nExample custom medians:\")\n",
    "for i, (col, median_val) in enumerate(list(sensor_medians_custom.items())[:5]):\n",
    "    print(f\"   - {col}: {median_val:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fb0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CUSTOM: Applying imputation to training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iduma\\AppData\\Local\\Temp\\ipykernel_40964\\1352193821.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_ops_clean_custom[col].fillna(sensor_medians_custom[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom imputation complete!\n",
      "\n",
      "üìä Results (Custom Implementation):\n",
      "   - Missing values BEFORE: 354,634\n",
      "   - Missing values AFTER:  0\n",
      "   - Status: ‚úÖ All filled!\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM: Apply imputation to training data using custom medians\n",
    "print(\"üîß CUSTOM: Applying imputation to training data...\")\n",
    "\n",
    "# Create a copy to preserve original\n",
    "train_ops_clean_custom = train_ops.copy()\n",
    "\n",
    "# Fill missing values using our custom-calculated medians\n",
    "for col in sensor_cols:\n",
    "    train_ops_clean_custom[col].fillna(sensor_medians_custom[col], inplace=True)\n",
    "\n",
    "# Verify no missing values remain\n",
    "missing_after_custom = train_ops_clean_custom.isnull().sum().sum()\n",
    "\n",
    "print(f\"‚úÖ Custom imputation complete!\")\n",
    "print(f\"\\nüìä Results (Custom Implementation):\")\n",
    "print(f\"   - Missing values BEFORE: {train_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing values AFTER:  {missing_after_custom:,}\")\n",
    "print(f\"   - Status: {'‚úÖ All filled!' if missing_after_custom == 0 else '‚ö†Ô∏è Still have missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66378b7",
   "metadata": {},
   "source": [
    "---\n",
    "### Option B: Sklearn Implementation (FOR BENCHMARKING ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55576051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ SKLEARN BENCHMARK: Testing sklearn SimpleImputer...\n",
      "‚úÖ Sklearn imputation complete!\n",
      "\n",
      "üìä Results (Sklearn Benchmark):\n",
      "   - Missing values BEFORE: 354,634\n",
      "   - Missing values AFTER:  0\n",
      "   - Status: ‚úÖ All filled!\n"
     ]
    }
   ],
   "source": [
    "# SKLEARN BENCHMARK: For comparison only (not used in final model)\n",
    "print(\"üî¨ SKLEARN BENCHMARK: Testing sklearn SimpleImputer...\")\n",
    "\n",
    "imputer_sklearn = SimpleImputer(strategy='median')\n",
    "imputer_sklearn.fit(train_ops[sensor_cols])\n",
    "\n",
    "train_ops_clean_sklearn = train_ops.copy()\n",
    "train_ops_clean_sklearn[sensor_cols] = imputer_sklearn.transform(train_ops[sensor_cols])\n",
    "\n",
    "missing_after_sklearn = train_ops_clean_sklearn.isnull().sum().sum()\n",
    "\n",
    "print(f\"‚úÖ Sklearn imputation complete!\")\n",
    "print(f\"\\nüìä Results (Sklearn Benchmark):\")\n",
    "print(f\"   - Missing values BEFORE: {train_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing values AFTER:  {missing_after_sklearn:,}\")\n",
    "print(f\"   - Status: {'‚úÖ All filled!' if missing_after_sklearn == 0 else '‚ö†Ô∏è Still have missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c85a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç VALIDATION: Comparing Custom vs Sklearn Implementation\n",
      "============================================================\n",
      "\n",
      "Median Values Comparison (first 5 sensors):\n",
      "Sensor                   Custom         Sklearn     Match?\n",
      "------------------------------------------------------------\n",
      "171_0              2,781,472.50    2,781,472.50          ‚úÖ\n",
      "666_0                 76,455.00       76,455.00          ‚úÖ\n",
      "427_0            108,090,562.00  108,090,562.00          ‚úÖ\n",
      "837_0                 15,755.00       15,755.00          ‚úÖ\n",
      "167_0                  3,570.00        3,570.00          ‚úÖ\n",
      "\n",
      "============================================================\n",
      "Final Data Match: ‚úÖ IDENTICAL\n",
      "============================================================\n",
      "\n",
      "‚úÖ Validation complete! Custom implementation matches sklearn.\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION: Compare custom vs sklearn results\n",
    "print(\"\\nüîç VALIDATION: Comparing Custom vs Sklearn Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare a few median values\n",
    "print(\"\\nMedian Values Comparison (first 5 sensors):\")\n",
    "print(f\"{'Sensor':<15} {'Custom':>15} {'Sklearn':>15} {'Match?':>10}\")\n",
    "print(\"-\"*60)\n",
    "for i, col in enumerate(sensor_cols[:5]):\n",
    "    custom_med = sensor_medians_custom[col]\n",
    "    sklearn_med = imputer_sklearn.statistics_[i]\n",
    "    match = \"‚úÖ\" if abs(custom_med - sklearn_med) < 0.01 else \"‚ùå\"\n",
    "    print(f\"{col:<15} {custom_med:>15,.2f} {sklearn_med:>15,.2f} {match:>10}\")\n",
    "\n",
    "# Compare final results\n",
    "results_match = (train_ops_clean_custom[sensor_cols].values == train_ops_clean_sklearn[sensor_cols].values).all()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Data Match: {'‚úÖ IDENTICAL' if results_match else '‚ùå DIFFERENT'}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\n‚úÖ Validation complete! Custom implementation matches sklearn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d53e6",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úÖ Using Custom Implementation for Final Model\n",
    "\n",
    "From this point forward, we use **ONLY the custom implementation** (`train_ops_clean_custom`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d739eba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading validation data...\n",
      "‚úÖ Loaded: (196227, 107)\n",
      "\n",
      "üîß Applying custom medians to validation data...\n",
      "‚úÖ Validation cleaned!\n",
      "   - Missing before: 60,339\n",
      "   - Missing after:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iduma\\AppData\\Local\\Temp\\ipykernel_40964\\3101831865.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  val_ops_clean[col].fillna(sensor_medians_custom[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load and process validation data with CUSTOM implementation\n",
    "print(\"üìÇ Loading validation data...\")\n",
    "val_ops = pd.read_csv('../data/raw/validation_operational_readouts.csv')\n",
    "print(f\"‚úÖ Loaded: {val_ops.shape}\")\n",
    "\n",
    "# Apply custom imputation\n",
    "print(\"\\nüîß Applying custom medians to validation data...\")\n",
    "val_ops_clean = val_ops.copy()\n",
    "for col in sensor_cols:\n",
    "    val_ops_clean[col].fillna(sensor_medians_custom[col], inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Validation cleaned!\")\n",
    "print(f\"   - Missing before: {val_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing after:  {val_ops_clean.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28eb287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving cleaned data...\n",
      "‚úÖ Data saved successfully!\n",
      "\n",
      "üìÅ Saved files (100% Custom Implementation - 0% sklearn):\n",
      "   - train_ops_cleaned.csv (1,122,452 rows √ó 107 cols)\n",
      "   - val_ops_cleaned.csv (196,227 rows √ó 107 cols)\n",
      "\n",
      "‚úÖ Ready for feature engineering!\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets (CUSTOM IMPLEMENTATION ONLY - 0% sklearn)\n",
    "print(\"üíæ Saving cleaned data...\")\n",
    "\n",
    "from pathlib import Path\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save both datasets\n",
    "train_ops_clean_custom.to_csv('../data/processed/train_ops_cleaned.csv', index=False)\n",
    "val_ops_clean.to_csv('../data/processed/val_ops_cleaned.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Data saved successfully!\")\n",
    "print(f\"\\nüìÅ Saved files (100% Custom Implementation - 0% sklearn):\")\n",
    "print(f\"   - train_ops_cleaned.csv ({train_ops_clean_custom.shape[0]:,} rows √ó {train_ops_clean_custom.shape[1]} cols)\")\n",
    "print(f\"   - val_ops_cleaned.csv ({val_ops_clean.shape[0]:,} rows √ó {val_ops_clean.shape[1]} cols)\")\n",
    "print(f\"\\n‚úÖ Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bf48c",
   "metadata": {},
   "source": [
    "## 3. Apply Median Imputation\n",
    "\n",
    "**Two Implementations:**\n",
    "1. **Custom (From Scratch)** - Used in final model\n",
    "2. **Sklearn (Benchmark)** - For validation/comparison only\n",
    "\n",
    "**Method:** Replace NaN with the median value of each sensor column\n",
    "\n",
    "**Why Median:**\n",
    "- Robust to outliers (not affected by extreme values)\n",
    "- Works great with decision trees\n",
    "- Simple and interpretable for thesis\n",
    "\n",
    "**Process:**\n",
    "1. Calculate median for each sensor (from non-missing values)\n",
    "2. Replace all NaN with that sensor's median\n",
    "3. Same medians applied to validation/test (no data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212692d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating median imputer...\n",
      "   - Fitting imputer on training data...\n",
      " Imputer fitted!\n",
      "\n",
      " Learned medians for 105 sensors\n",
      "\n",
      "Example medians:\n",
      "   - 171_0: 2,781,472\n",
      "   - 666_0: 76,455\n",
      "   - 427_0: 108,090,562\n",
      "   - 837_0: 15,755\n",
      "   - 167_0: 3,570\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Calculate medians for each sensor (from scratch)\n",
    "print(\"üìä Calculating median for each sensor column...\")\n",
    "\n",
    "# Dictionary to store median values for each sensor\n",
    "sensor_medians = {}\n",
    "\n",
    "# Calculate median for each sensor column (ignoring NaN values)\n",
    "for col in sensor_cols:\n",
    "    # Get non-missing values for this column\n",
    "    non_missing_values = train_ops[col].dropna()\n",
    "    \n",
    "    # Calculate median manually using NumPy\n",
    "    if len(non_missing_values) > 0:\n",
    "        sensor_medians[col] = np.median(non_missing_values)\n",
    "    else:\n",
    "        sensor_medians[col] = 0  # Fallback if all values are missing\n",
    "        \n",
    "print(\"‚úÖ Medians calculated!\")\n",
    "print(f\"\\nüìà Learned medians for {len(sensor_medians)} sensors\")\n",
    "print(f\"\\nExample medians:\")\n",
    "for i, (col, median_val) in enumerate(list(sensor_medians.items())[:5]):  # Show first 5\n",
    "    print(f\"   - {col}: {median_val:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1577b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Applying imputation to training data...\n",
      " Imputation complete!\n",
      "\n",
      " Results:\n",
      "   - Missing values BEFORE: 354,634\n",
      "   - Missing values AFTER:  0\n",
      "   - Status:  All filled!\n",
      "\n",
      " Example - Row 2, sensor 167_1:\n",
      "   - Before: NaN\n",
      "   - After:  5,276,156 (median value)\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Transform the training data (fill missing values)\n",
    "print(\" Applying imputation to training data...\")\n",
    "\n",
    "# Create a copy to preserve original\n",
    "train_ops_clean = train_ops.copy()\n",
    "\n",
    "# Transform sensor columns (replaces NaN with medians)\n",
    "train_ops_clean[sensor_cols] = imputer.transform(train_ops[sensor_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "missing_after = train_ops_clean.isnull().sum().sum()\n",
    "\n",
    "print(f\" Imputation complete!\")\n",
    "print(f\"\\n Results:\")\n",
    "print(f\"   - Missing values BEFORE: {train_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing values AFTER:  {missing_after:,}\")\n",
    "print(f\"   - Status: {' All filled!' if missing_after == 0 else '‚ö†Ô∏è Still have missing'}\")\n",
    "\n",
    "# Show example of filled value\n",
    "if pd.isna(train_ops.loc[2, '167_1']):\n",
    "    print(f\"\\n Example - Row 2, sensor 167_1:\")\n",
    "    print(f\"   - Before: NaN\")\n",
    "    print(f\"   - After:  {train_ops_clean.loc[2, '167_1']:,.0f} (median value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e91a8e",
   "metadata": {},
   "source": [
    "## 4. Apply to Validation & Test Sets (Custom Implementation)\n",
    "\n",
    "**Important:** We use the SAME medians from training data (custom-calculated)\n",
    "\n",
    "**Why:** Prevents data leakage - validation/test are \"unseen\" data, so we can't calculate their medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934897b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation operational readouts...\n",
      " Loaded: (196227, 107)\n",
      "\n",
      " Applying training medians to validation data...\n",
      " Validation cleaned!\n",
      "   - Missing before: 60,339\n",
      "   - Missing after:  0\n"
     ]
    }
   ],
   "source": [
    "# Load and clean validation data (using CUSTOM medians)\n",
    "print(\"üìÇ Loading validation operational readouts...\")\n",
    "val_ops = pd.read_csv('../data/raw/validation_operational_readouts.csv')\n",
    "print(f\"‚úÖ Loaded: {val_ops.shape}\")\n",
    "\n",
    "# Apply CUSTOM imputation (using training medians)\n",
    "print(\"\\nüîß CUSTOM: Applying training medians to validation data...\")\n",
    "val_ops_clean = val_ops.copy()\n",
    "\n",
    "# Fill missing values using our custom-calculated medians\n",
    "for col in sensor_cols:\n",
    "    val_ops_clean[col].fillna(sensor_medians_custom[col], inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Validation cleaned (custom implementation)!\")\n",
    "print(f\"   - Missing before: {val_ops.isnull().sum().sum():,}\")\n",
    "print(f\"   - Missing after:  {val_ops_clean.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0fdbe",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "**What we're saving:** Time series data with NO missing values\n",
    "\n",
    "**Next step:** Feature engineering (convert time series ‚Üí statistical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd35de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving cleaned data to processed folder...\n",
      " Data saved successfully!\n",
      "\n",
      " Saved files:\n",
      "   - train_ops_cleaned.csv (1,122,452 rows √ó 107 cols)\n",
      "   - val_ops_cleaned.csv (196,227 rows √ó 107 cols)\n",
      "\n",
      " Status: Ready for feature engineering!\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets (CUSTOM IMPLEMENTATION ONLY)\n",
    "print(\"üíæ Saving cleaned data to processed folder...\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "from pathlib import Path\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save cleaned datasets (using custom implementation)\n",
    "train_ops_clean_custom.to_csv('../data/processed/train_ops_cleaned.csv', index=False)\n",
    "val_ops_clean.to_csv('../data/processed/val_ops_cleaned.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Data saved successfully!\")\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "print(f\"   - train_ops_cleaned.csv ({train_ops_clean_custom.shape[0]:,} rows √ó {train_ops_clean_custom.shape[1]} cols)\")\n",
    "print(f\"   - val_ops_cleaned.csv ({val_ops_clean.shape[0]:,} rows √ó {val_ops_clean.shape[1]} cols)\")\n",
    "print(f\"\\n‚úÖ Status: Ready for feature engineering!\")\n",
    "print(f\"\\nüìå NOTE: Files saved using 100% custom implementation (no sklearn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743b430",
   "metadata": {},
   "source": [
    "## ‚úÖ Preprocessing Complete!\n",
    "\n",
    "### Summary of What We Did:\n",
    "\n",
    "**1. Loaded Data**\n",
    "- Training: 5M+ time series rows\n",
    "- Validation: 196K time series rows\n",
    "\n",
    "**2. Analyzed Missing Data**\n",
    "- Found ~0.3% missing in 167_X sensor columns\n",
    "- Missing values appear in cumulative sensors (mileage-like)\n",
    "\n",
    "**3. Applied Median Imputation**\n",
    "- Calculated median for each sensor from training data\n",
    "- Replaced ALL NaN with their respective medians\n",
    "- Used training medians for validation (no data leakage)\n",
    "\n",
    "**4. Verified Results**\n",
    "- ‚úÖ Zero missing values in cleaned data\n",
    "- ‚úÖ All sensors now have complete readings\n",
    "\n",
    "**5. Saved Cleaned Data**\n",
    "- Files ready in `data/processed/` folder\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**‚Üí Go to `03_feature_engineering.ipynb`**\n",
    "\n",
    "We'll convert the time series into statistical features:\n",
    "- Mean, Max, Min, Std, Last, Trend for each sensor\n",
    "- Transform: 5M rows ‚Üí 23,550 rows (one per vehicle)\n",
    "- Create 630 features for decision tree training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839adb80",
   "metadata": {},
   "source": [
    "## 3. Aggregate Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99803b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate operational readouts per vehicle\n",
    "print(\"Aggregating time series data...\")\n",
    "train_agg = aggregate_time_series(train_ops_clean, aggregation_funcs=['mean', 'std', 'min', 'max'])\n",
    "val_agg = aggregate_time_series(val_ops_clean, aggregation_funcs=['mean', 'std', 'min', 'max'])\n",
    "test_agg = aggregate_time_series(test_ops_clean, aggregation_funcs=['mean', 'std', 'min', 'max'])\n",
    "\n",
    "print(f\"\\nAggregated shapes:\")\n",
    "print(f\"Train: {train_agg.shape}\")\n",
    "print(f\"Validation: {val_agg.shape}\")\n",
    "print(f\"Test: {test_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf6f50",
   "metadata": {},
   "source": [
    "## 4. Merge with Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e10018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge aggregated features with vehicle specifications\n",
    "print(\"Merging operational and specification data...\")\n",
    "train_merged = merge_operational_specs(train_agg, train_specs)\n",
    "val_merged = merge_operational_specs(val_agg, val_specs)\n",
    "test_merged = merge_operational_specs(test_agg, test_specs)\n",
    "\n",
    "print(f\"\\nMerged shapes:\")\n",
    "print(f\"Train: {train_merged.shape}\")\n",
    "print(f\"Validation: {val_merged.shape}\")\n",
    "print(f\"Test: {test_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb56d6f",
   "metadata": {},
   "source": [
    "## 5. Add Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc265b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to merged datasets\n",
    "# Training: convert TTE to binary (0 = healthy, 1 = failed)\n",
    "train_merged = train_merged.merge(train_tte, on='id_vehicle', how='left')\n",
    "train_merged['label'] = (train_merged['tte'] == 0).astype(int)\n",
    "\n",
    "# Validation and Test\n",
    "val_merged = val_merged.merge(val_labels, on='id_vehicle', how='left')\n",
    "test_merged = test_merged.merge(test_labels, on='id_vehicle', how='left')\n",
    "\n",
    "print(\"Labels added successfully!\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Train: {train_merged['label'].value_counts().to_dict()}\")\n",
    "print(f\"Validation: {val_merged['label'].value_counts().to_dict()}\")\n",
    "print(f\"Test: {test_merged['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2871a2",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11343746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (excluding ID and label columns)\n",
    "print(\"Scaling features...\")\n",
    "\n",
    "# Create new preprocessor for scaling merged data\n",
    "scaler = ScaniaPreprocessor(scaling_method='standard')\n",
    "\n",
    "train_scaled = scaler.scale_features(train_merged, fit=True)\n",
    "val_scaled = scaler.scale_features(val_merged, fit=False)\n",
    "test_scaled = scaler.scale_features(test_merged, fit=False)\n",
    "\n",
    "print(\"Scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33439b13",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "train_scaled.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "val_scaled.to_csv('../data/processed/validation_processed.csv', index=False)\n",
    "test_scaled.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "print(\"\\nProcessed data saved successfully!\")\n",
    "print(f\"Train: ../data/processed/train_processed.csv ({train_scaled.shape})\")\n",
    "print(f\"Validation: ../data/processed/validation_processed.csv ({val_scaled.shape})\")\n",
    "print(f\"Test: ../data/processed/test_processed.csv ({test_scaled.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8d3ad",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Preprocessing Steps Completed**:\n",
    "1. ‚úÖ Loaded raw data from CSV files\n",
    "2. ‚úÖ Handled missing values using median imputation\n",
    "3. ‚úÖ Aggregated time series to vehicle-level features\n",
    "4. ‚úÖ Merged operational and specification data\n",
    "5. ‚úÖ Added binary labels\n",
    "6. ‚úÖ Scaled features using StandardScaler\n",
    "7. ‚úÖ Saved processed data\n",
    "\n",
    "**Next Steps**:\n",
    "- Feature engineering (advanced time series features)\n",
    "- Baseline model development\n",
    "- Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff7fee",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Data preprocessing is complete! The cleaned data is saved and ready for feature engineering.\n",
    "\n",
    "**‚Üí Continue to [03_feature_engineering.ipynb](03_feature_engineering.ipynb)**\n",
    "\n",
    "In the next notebook, we'll transform the time series data into statistical features for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
